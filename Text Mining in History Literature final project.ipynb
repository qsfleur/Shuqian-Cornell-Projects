{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTrEW88PuqC9"
   },
   "source": [
    "# Final project\n",
    "\n",
    "### The task\n",
    "\n",
    "Your task is to: identify an interesting problem that's addressable with the help of computational methods applied to the supplied corpus, formulate a hypothesis about that problem, devise an experiment or experiments to test your hypothesis, present the results of your investigations, and discuss your findings.\n",
    "\n",
    "This workflow essentially replicates the process of writing an academic paper. You can think of your exam as a paper in miniature.\n",
    "\n",
    "You are free to present each of these tasks as you see fit. You should use narrative text (that is, your own writing in a markdown cell), citations of others' work, numerical results, tables of data, and static and/or interactive visualizations as appropriate. Total length is flexible and depends on the number of people involved in the work, as well as the specific balance you strike between the ambition of your question and the sophistication of your methods. But be aware that numbers never, ever speak for themselves. Quantitative results presented without substantial discussion will not earn high marks.\n",
    "\n",
    "Your project should reflect, at minimum, ten **or more** hours of work, though you will be graded on the quality of your output, not the amount of time it took you to produce it. Most high-quality projects represent twenty or more hours of work.\n",
    "\n",
    "#### Pick an important and interesting problem!\n",
    "\n",
    "No amount of technical sophistication will overcome a fundamentally uninteresting problem at the core of your work. You have seen many pieces of successful computational humanities research over the course of the semester. You might use these as a guide to the kinds of problems that interest scholars in a range of humanities disciplines. You may also want to spend some time in the library, reading recent books and articles in the professional literature. **Problem selection and motivation are integral parts of the project.** Do not neglect them.\n",
    "\n",
    "### The corpus\n",
    "\n",
    "We have supplied you (via the course GitHub site) with a corpus of 1,540 volumes of American fiction published between 1789 and 1875, as well as a range of potentially relevant metadata. This corpus is large: it contains well over 100 million words. Some summary and descriptive statistics are included below, along with a short annotation of the metadata fields.\n",
    "\n",
    "**Be aware that some (but certainly not all) text analysis tasks will be slow (or impossible) when run over a corpus as large as this one.** For comparison purposes, the subsampled album review dataset we used for homework 8 contained about 1% as many words (but a similar number of total documents). You might consider whether or not your question requires the use of the full corpus.\n",
    "\n",
    "Books in the corpus are those that were included in volumes 1 and 2 of Lyle Wright's three-volume bibliography of American fiction before 1900 and that were digitized by the University of Virginia (1789-1850) and Indiana University (1851-1875). This corpus includes about 40% of the American fiction from the period (1789-1875) that has been preserved in American academic libraries. You might think a little about what kinds of books are most likely to have found their way first into print and then into academic libraries, and what kinds of books (and authors) might not have.\n",
    "\n",
    "Metadata were collected manually by a team of undergraduate students at the University of Notre Dame.\n",
    "\n",
    "**Note that the nineteenth century was awful.** These books reflect that fact in all kinds of ways, even though (or maybe because) they were generally considered unproblematic at the time. If you read the books or dig very far into the most informative features, you will quickly discover objectionable content. It would be valuable to devise (and you will be rewarded for devising) methods to avoid displaying unmasked versions of racial slurs, for example, in any visualization that might otherwise include them.\n",
    "\n",
    "### Format\n",
    "\n",
    "You should submit your exam as a report in the form of a Jupyter notebook that includes all code, figures, and write-up.\n",
    "\n",
    "Your report should have four basic sections (provided in cells below for ease of reference and reuse):\n",
    "\n",
    "1. **Introduction and hypothesis.** What problem are you working on? Why is it interesting and important? What have other people said about it? What do you expect to find?\n",
    "2. **Corpus, data, and methods.** What data have you used? What are the limitations of that data? What major methods will you use to analyze it? Why are those methods the appropriate ones?\n",
    "3. **Results.** What did you find? How did you find it? How should we read your figures? Be sure to include confidence intervals or other measures of statistical significance or uncetainty where appropriate.\n",
    "4. **Discussion and conclusions.** What does it all mean? Do your results support your hypothesis? Why or why not? What are the limitations of your study and how might those limitations be addressed in future work?\n",
    "\n",
    "Within each of those sections, you may use as many code and markdown cells as you like. You may, of course, address additional questions or issues not listed above. You may also gather additional data or metadata relevant to your analysis, but you are not required to do so.\n",
    "\n",
    "All code used in the project should be present in the notebook (except for widely-available libraries that you import), but **be sure that we can read and understand your report in full without rerunning the code**. Unexecuted code will receive no credit. Be sure, too, to explain what you're doing along the way, both by describing your data and methods and by writing clean, well commented code.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This exam is the take-home final for the course. It is worth 35% of your overall grade. You will be graded on the quality and ambition of each aspect of the project. No single component is more important than the others.\n",
    "\n",
    "### Practical details\n",
    "\n",
    "* The exam is due at **noon on Saturday, December 9** via upload of a single, fully executed Jupyter notebook file to CMS.\n",
    "* **You must work alone.** You may not collaborate with others.\n",
    "    * You may post questions on Ed, but should do so privately (visible to course staff only).\n",
    "* Interactive visualizations do not always work when embedded in shared notebooks. If you plan to use interactives, you may need to host them elsewhere and link to them.\n",
    "\n",
    "---"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVidzjOruqDD"
   },
   "source": [
    "## 1. Introduction and hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGYx6TLHuqDD"
   },
   "source": [
    "I will work on <b>the relationship between the authors' occupations and their writing</b>, specifically, <b>how authors whose primary occupation as Church, Politics-Government-Activism and Law could potentially relate</b>. This is an interesting question because, between 1789 and 1875, the founding years of the United States, the churches played a central role in American lives and the church figures had a significant presence across the country. They provided spiritual guidance and shaped religious beliefs for people and communities. They played a central role in influencing social norms and values and advocating for politically-related things such as abolitionism. The legal frameworks are also influenced by church and religious beliefs. The close relationship between Politics-Government-Activism and Law is explicit, as these two things are inherently relevant. I am interested in whether texts written by people working in the field of Politics-Government-Activism and Law reflect the religious influence, and whether writers from church backgrounds write about Politics-Government-Activism and Law.\n",
    "\n",
    "Questions I will ask throughout the project are: Are writers' occupations and real-world experience reflected in their writing? Do writers with occupations in church address issues in Politics-Government-Activism and Law within their written works, and do authors in Politics-Government-Activism and Law have religious impacts in their writing? How do we distinguish texts written by authors in Politics-Government-Activism and Law? What are the top word choices they use that indicate the mutual influence? Could we predict the author's occupation (Politics-Government-Activism, Law, or Church) based on their writing? The ultimate goal of this project is to train a model to predict the writer's occupation based on the writing using LLM-based model, and maximum accuracy is desired.\n",
    "\n",
    "For this project, I expect to find that writers who share similar occupations and prefer writing similar topics and use similar words. I also expect to find high accuracies to predict the author's occupation through their pieces of writing. If successful, this project could have a meaningful extension to the modern world in exploring writing preference similarities between authors who have occupations in similar areas and different occupations are closely interrelated that are reflected in their writing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euYw-TiEuqDE"
   },
   "source": [
    "## 2. Data and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N9xLmAzNqah8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klf-ZeVN4Uk-",
    "outputId": "3a500210-7a39-49a8-e33e-d93139b22a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k84LwiX_4UYX",
    "outputId": "ce0fa814-d9e8-4b12-dbbc-9faf2f2cc5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YwygvvI-uqDE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import fightinwords as fw\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from   sklearn.linear_model import LogisticRegression\n",
    "from   sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from   transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from torch.utils.data import DataLoader\n",
    "from   collections import defaultdict\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n2G0zFrGuqDF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File locations\n",
    "#   Note that metadata are supplied as a TSV file\n",
    "#   Text files are in a directory, one file per (long, novel-like) document\n",
    "metadata_file = os.path.join('data', 'us_fiction', 'corpus_data.tsv')\n",
    "text_dir      = os.path.join('data', 'us_fiction', 'us_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bqwI463UuqDF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "metadata = pd.read_csv(\n",
    "    metadata_file,\n",
    "    sep='\\t',\n",
    "    low_memory=False\n",
    ").set_index('source_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA2Nytg7uqDF"
   },
   "source": [
    "### Corpus details\n",
    "\n",
    "The cells below are supplied to help you understand the corpus. **You should remove them from your completed exam** and include only the information you deem relevant to your report. That said, you are free to keep the metadata-loading code above and you may copy any and all of the other code below for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XQW4mHCuuqDG",
    "outputId": "b47378b4-aa32-4006-f5a6-95b8697cc058",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e903f4d1-7965-48ad-9136-9f3d96ace12e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pub_place</th>\n",
       "      <th>publisher</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_guess</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_free</th>\n",
       "      <th>state_born</th>\n",
       "      <th>state_main</th>\n",
       "      <th>state_died</th>\n",
       "      <th>born</th>\n",
       "      <th>died</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eaf001</th>\n",
       "      <td>Allston, Washington</td>\n",
       "      <td>Monaldi</td>\n",
       "      <td>Boston</td>\n",
       "      <td>C. C. Little and J. Brown</td>\n",
       "      <td>1841</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Painter</td>\n",
       "      <td>SC</td>\n",
       "      <td>MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>1779.0</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>47541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaf002</th>\n",
       "      <td>Bacon, Delia Salter</td>\n",
       "      <td>Tales of the puritans</td>\n",
       "      <td>New Haven [Conn.]</td>\n",
       "      <td>A. H. Maltby</td>\n",
       "      <td>1831</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Education</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>OH</td>\n",
       "      <td>CT</td>\n",
       "      <td>CT</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>70010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaf003</th>\n",
       "      <td>Bacon, Delia Salter</td>\n",
       "      <td>Love's martyr</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>Printed by E. Morgan and Co.</td>\n",
       "      <td>1838</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Education</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>OH</td>\n",
       "      <td>CT</td>\n",
       "      <td>CT</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>13547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaf004</th>\n",
       "      <td>Bacon, Delia Salter</td>\n",
       "      <td>The bride of Fort Edward</td>\n",
       "      <td>New York</td>\n",
       "      <td>Samuel Colman</td>\n",
       "      <td>1839</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Education</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>OH</td>\n",
       "      <td>CT</td>\n",
       "      <td>CT</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>34309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaf005</th>\n",
       "      <td>Belknap, Jeremy</td>\n",
       "      <td>The foresters</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Printed by I. Thomas and E. T. Andrews</td>\n",
       "      <td>1792</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Church</td>\n",
       "      <td>Minister</td>\n",
       "      <td>MA</td>\n",
       "      <td>NH</td>\n",
       "      <td>MA</td>\n",
       "      <td>1744.0</td>\n",
       "      <td>1798.0</td>\n",
       "      <td>33731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e903f4d1-7965-48ad-9136-9f3d96ace12e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e903f4d1-7965-48ad-9136-9f3d96ace12e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e903f4d1-7965-48ad-9136-9f3d96ace12e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-a24b061f-5ab5-4b54-a4bc-5101862738e6\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a24b061f-5ab5-4b54-a4bc-5101862738e6')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-a24b061f-5ab5-4b54-a4bc-5101862738e6 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                        author                     title          pub_place  \\\n",
       "source_id                                                                     \n",
       "eaf001     Allston, Washington                   Monaldi             Boston   \n",
       "eaf002     Bacon, Delia Salter     Tales of the puritans  New Haven [Conn.]   \n",
       "eaf003     Bacon, Delia Salter             Love's martyr         Cincinnati   \n",
       "eaf004     Bacon, Delia Salter  The bride of Fort Edward           New York   \n",
       "eaf005         Belknap, Jeremy             The foresters             Boston   \n",
       "\n",
       "                                        publisher  pub_date gender  \\\n",
       "source_id                                                            \n",
       "eaf001                  C. C. Little and J. Brown      1841      M   \n",
       "eaf002                               A. H. Maltby      1831      F   \n",
       "eaf003               Printed by E. Morgan and Co.      1838      F   \n",
       "eaf004                              Samuel Colman      1839      F   \n",
       "eaf005     Printed by I. Thomas and E. T. Andrews      1792      M   \n",
       "\n",
       "           gender_guess ethnicity occupation occupation_free state_born  \\\n",
       "source_id                                                                 \n",
       "eaf001              0.0     White       Arts         Painter         SC   \n",
       "eaf002              0.0     White  Education         Teacher         OH   \n",
       "eaf003              0.0     White  Education         Teacher         OH   \n",
       "eaf004              0.0     White  Education         Teacher         OH   \n",
       "eaf005              0.0     White     Church        Minister         MA   \n",
       "\n",
       "          state_main state_died    born    died  words  \n",
       "source_id                                               \n",
       "eaf001            MA         MA  1779.0  1843.0  47541  \n",
       "eaf002            CT         CT  1811.0  1859.0  70010  \n",
       "eaf003            CT         CT  1811.0  1859.0  13547  \n",
       "eaf004            CT         CT  1811.0  1859.0  34309  \n",
       "eaf005            NH         MA  1744.0  1798.0  33731  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glance at the metadata\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ANAw_ZuquqDH",
    "outputId": "39d97236-a444-4210-d95d-38351165689d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ee5ffb5d-1a28-4ccc-8ca6-44c7a29c881a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>gender_guess</th>\n",
       "      <th>born</th>\n",
       "      <th>died</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1540.000000</td>\n",
       "      <td>1425.000000</td>\n",
       "      <td>1188.000000</td>\n",
       "      <td>1165.000000</td>\n",
       "      <td>1.540000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1853.963636</td>\n",
       "      <td>0.197193</td>\n",
       "      <td>1811.624579</td>\n",
       "      <td>1878.185408</td>\n",
       "      <td>7.584156e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.764799</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>17.669353</td>\n",
       "      <td>22.077845</td>\n",
       "      <td>5.916979e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1789.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1744.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1.667000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1845.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1804.000000</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>4.820550e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1855.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1811.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>6.927450e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1867.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1823.000000</td>\n",
       "      <td>1893.000000</td>\n",
       "      <td>9.377150e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1875.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1857.000000</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1.150556e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee5ffb5d-1a28-4ccc-8ca6-44c7a29c881a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ee5ffb5d-1a28-4ccc-8ca6-44c7a29c881a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ee5ffb5d-1a28-4ccc-8ca6-44c7a29c881a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-127cf3ba-5c8e-4f77-87b1-f37c5f3ff507\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-127cf3ba-5c8e-4f77-87b1-f37c5f3ff507')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-127cf3ba-5c8e-4f77-87b1-f37c5f3ff507 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          pub_date  gender_guess         born         died         words\n",
       "count  1540.000000   1425.000000  1188.000000  1165.000000  1.540000e+03\n",
       "mean   1853.963636      0.197193  1811.624579  1878.185408  7.584156e+04\n",
       "std      15.764799      0.398019    17.669353    22.077845  5.916979e+04\n",
       "min    1789.000000      0.000000  1744.000000  1793.000000  1.667000e+03\n",
       "25%    1845.000000      0.000000  1804.000000  1860.000000  4.820550e+04\n",
       "50%    1855.000000      0.000000  1811.000000  1881.000000  6.927450e+04\n",
       "75%    1867.000000      0.000000  1823.000000  1893.000000  9.377150e+04\n",
       "max    1875.000000      1.000000  1857.000000  1934.000000  1.150556e+06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary stats for numeric columns\n",
    "metadata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHagIYo8uqDH"
   },
   "source": [
    "### Field definitions and distributional stats\n",
    "\n",
    "Most of the metadata fields are self-explanatory, but here are some details. Note that not every field in the metadata is described below.\n",
    "\n",
    "* `source_id`: This is the name of the file corresponding to the volume. You can use it to match metadata records to full-text documents. Note that the corpus includes a nontrivial number of multivolume works. These volumes have `source_id`s like `eaf086v1` or `Wright2-1720v2`.\n",
    "* `gender`: Author gender. `M`, `F`, or  `NaN` (= unknown).\n",
    "* `gender_guess`: Was the author gender assignment determined by biographical research (`0`) or by guessing on the basis of the author's name (`1`)?\n",
    "* `ethnicity`: Author ethnicity. One of `White`, `Black`, `Native`, or `NaN` (= unknown). Always assigned via biographical research. Not very useful, as the values are almost exclusively `White` or `NaN`. This fact tells you something about the US literary field in the nineteenth century.\n",
    "* `occupation` and `occupation_free`: The author's primary employment identification. Recall that the US in the nineteenth century didn't always have a large market for novels, so many of the authors in the corpus made their living by other means. The difference between these fields is that `occupation` uses a fixed vocabulary, while `occupation_free` does not (so includes more detailed or fine-grained information).\n",
    "* `state_*`: The state in which the author was `born`, `died`, and with which they are conventionally associated (`main`).\n",
    "* `born` and `died`: Year of the author's birth and death, respectively, where known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "T7WQUgIduqDH",
    "outputId": "142890e0-c28b-40f7-d046-aa1de3f4cd23",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Editor-Critic-Journalist        217\n",
       "Writer                          187\n",
       "Education                       147\n",
       "Politics-Government-Activism    136\n",
       "Church                          121\n",
       "Military-Seafaring               72\n",
       "Law                              57\n",
       "Business-Trade                   33\n",
       "Medicine                         28\n",
       "Arts                              6\n",
       "Agriculture-Land                  6\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Occurrence counts for selected metadata fields\n",
    "for col in ['occupation']:\n",
    "    display(metadata[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ijdgCOhDuqDI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of publication dates\n",
    "# metadata.pub_date.plot.hist(bins=metadata.pub_date.max()-metadata.pub_date.min()+1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QKckyOCsuqDI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of volume lengths\n",
    "#   Note removal of long volumes from vis\n",
    "# metadata.loc[metadata.words.between(0,250000)].words.plot.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ig9HIitHuqDI",
    "outputId": "da97e423-2719-45f2-ac1f-6c6bd008a3ff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the processed dataframe: (927, 16)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: process texts, remove redundant rows, and put labels and texts into lists\n",
    "texts_directory = 'data/us_fiction/us_texts'\n",
    "file_names = os.listdir(texts_directory)\n",
    "\n",
    "filtered_md = metadata[metadata.index.isin(file_names)]\n",
    "\n",
    "print(\"Shape of the processed dataframe:\", filtered_md.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkc5PuubuqDJ",
    "outputId": "c0955cf2-d739-4768-dc6f-63e790811130",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts for each occupation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Politics-Government-Activism    55\n",
       "Church                          33\n",
       "Law                             25\n",
       "Name: occupation, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts for each occupation\n",
    "print(\"\\nCounts for each occupation:\")\n",
    "\n",
    "occp=['Law','Church', 'Politics-Government-Activism']\n",
    "fmd = filtered_md[filtered_md['occupation'].isin(occp)]\n",
    "fmd.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1gCUtwquqDJ"
   },
   "source": [
    "Based on the output, there are 58 texts from authors in Politics-Government-Activism, 33 texts from authors in Church, 27 texts fro authors in Law. It's a pretty small corpora and the number of texts for each occupation differs, though not a lot. This corpus is still usable for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2tMjq9pKuqDJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process all occupations and put them into list\n",
    "labels = []\n",
    "for item in fmd['occupation'].items():\n",
    "    labels.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xwCgTf6CuqDK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# index list for texts\n",
    "labels_index = []\n",
    "\n",
    "for item in filtered_md['occupation'].items():\n",
    "    if str(item[1]) in [\"Church\", \"Law\", \"Politics-Government-Activism\"]:\n",
    "        labels_index.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsCf7M73uqDK",
    "outputId": "2ed93a0a-0f92-4365-aa7c-9f1ceee3138f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the processed text:\n",
      "Wright2-0618\n",
      "PROLOGUE.\n",
      "COLONEL SURRY TO THE READER:\n",
      "I perform a bold exploit to-day, my dear reader.\n",
      "The exploit in question is sending \n",
      "Hilt to Hilt\n",
      " to the press.\n",
      "It is a long time now since 1866, and, if you have read, you have probably forgotten the volume entitled \n",
      "Surry of Eagle's Nest.\n",
      "Alas! authors must expect to be lost sight of as the years flow on. I am not so vain as to imagine you remember my memoirs; and, for a stronger reason still, you must have forgotten their reception by my critical friend\n"
     ]
    }
   ],
   "source": [
    "# process texts from the indices that contain a valid occupation\n",
    "texts = {}\n",
    "file_names = []\n",
    "file_path = ''\n",
    "for file_name in os.listdir(texts_directory):\n",
    "    if file_name in labels_index:\n",
    "        file_path = os.path.join(texts_directory, file_name)\n",
    "        file_names.append(file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            texts[file_name] = text\n",
    "\n",
    "print(\"Preview of the processed text:\")\n",
    "first_key = next(iter(texts))\n",
    "first_value = next(iter(texts.values()))\n",
    "print(first_key)\n",
    "print(first_value[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2myj2KthuqDK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-organize into a new dictionary that contains combined texts from the three occupations\n",
    "church_names = []\n",
    "politics_names = []\n",
    "law_names = []\n",
    "combined = {\"Church\": \"\", \"Politics-Government-Activism\": \"\", \"Law\": \"\"}\n",
    "i = 0\n",
    "for ind in labels_index:\n",
    "    if labels[i] == \"Church\":\n",
    "        church_names.append(ind)\n",
    "        i += 1\n",
    "    elif labels[i] == \"Politics-Government-Activism\":\n",
    "        politics_names.append(ind)\n",
    "        i += 1\n",
    "    elif labels[i] == \"Law\":\n",
    "        law_names.append(ind)\n",
    "        i += 1\n",
    "\n",
    "for key, value in texts.items():\n",
    "    if key in church_names:\n",
    "        combined[\"Church\"] += value + \" \"\n",
    "    elif key in politics_names:\n",
    "        combined[\"Politics-Government-Activism\"] += value + \" \"\n",
    "    elif key in law_names:\n",
    "        combined[\"Law\"] += value + \" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_IJbOrSuqDK"
   },
   "source": [
    "Now, I will use fightinwords to compare top word choices across texts written by authors from the three occupations. Stopwords will be removed. I will use a CountVectorizer to process the text to a matrix that contains occurrences of different words in each occupation category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E9GSxAuuqDL",
    "outputId": "f0f5749e-f9c2-4414-8cac-e7fc54bd9684",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['00' '000' '006' ... 'zum' 'zusts' 'zyness']\n"
     ]
    }
   ],
   "source": [
    "# process the text using countvectorizer\n",
    "\n",
    "prefw_vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    stop_words = 'english'\n",
    ")\n",
    "\n",
    "vectorized_text = prefw_vectorizer.fit_transform([combined[name] for name in occp])\n",
    "\n",
    "feature_names = prefw_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Feature names: \", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BJm5hGiJ0_l_"
   },
   "outputs": [],
   "source": [
    "# stopword list from pset1\n",
    "stopword = ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren',\n",
    "            \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "            'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don',\n",
    "            \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn',\n",
    "            \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself',\n",
    "            'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll',\n",
    "            'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\",\n",
    "            'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "            'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "            \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "            'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very',\n",
    "            'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who',\n",
    "            'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\",\n",
    "            \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "# define remove_stopwords function\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "I4FvjffM0_yf"
   },
   "outputs": [],
   "source": [
    "# process texts from the same occupation category, remove stopwords\n",
    "def random_text_occupation(texts, occupation1, occupation2, random_num=2):\n",
    "    output_1 = \"\"\n",
    "    output_2 = \"\"\n",
    "    for key, text in texts.items():\n",
    "        if key == occupation1:\n",
    "            output_1 += text\n",
    "        elif key == occupation2:\n",
    "            output_2 += text\n",
    "    for texts in output_1:\n",
    "        for text in texts:\n",
    "            text = remove_stopwords(text, stopword)\n",
    "    for texts in output_2:\n",
    "        for text in texts:\n",
    "            text = remove_stopwords(text, stopword)\n",
    "    print(output_1)\n",
    "    text_1 = [fw.basic_sanitize(line) for line in output_1.split('\\n') if line.strip()]\n",
    "    text_2 = [fw.basic_sanitize(line) for line in output_2.split('\\n') if line.strip()]\n",
    "    flat = fw.bayes_compare_language(text_1, text_2)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CZMw1o1X0_7U"
   },
   "outputs": [],
   "source": [
    "# display top 10 words in the two corpora\n",
    "def display_fw(data, n=10, name1='corpus one', name2='corpus two'):\n",
    "    '''Display the indicated number of top terms from fightinwords output.'''\n",
    "    print(\"Top terms in\", name1)\n",
    "    for term, score in reversed(data[-n:]):\n",
    "        print(f\"{term:<10} {score:6.3f}\")\n",
    "    print(\"\")\n",
    "    print(\"Top terms in\", name2)\n",
    "    for term, score in data[:n]:\n",
    "        print(f\"{term:<10} {score:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzNht_srS0NK",
    "outputId": "3706cbd8-fb6f-44e2-d24a-a4a8bfab6c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence:  ['Law', 'Church', 'Politics-Government-Activism']\n"
     ]
    }
   ],
   "source": [
    "# determine which number represents which occupation\n",
    "print(\"Sequence: \", occp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POevpS9auqDL"
   },
   "source": [
    "For clarification, based on the output above, since earlier in the code, when I vectorized the text, I iterated over the list occp, the 0, 1, and 2 represents the indices of the occupations in occp, as below:\n",
    "[0] is Law, [1] is church, [2] is Politics-Government-Activism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM2I8xOfbEbY"
   },
   "source": [
    "For the priors for the fightinwords analyses, I will use the default prior = 0.01. The default value works here because I need a uniform baseline for all three cross-comparisons between two occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7mNzji6uqDL",
    "outputId": "c05bafdd-4059-488b-fc35-d243389e6dde",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Law and Politics-Government-Activism occupations\n",
      "Corpus one: Law\n",
      "Corpus two: Politics-Government-Activism\n",
      "\n",
      "Vocab size is 64188\n",
      "Comparing language...\n",
      "Top terms in corpus one\n",
      "sir        59.443\n",
      "captain    41.478\n",
      "said       37.281\n",
      "general    34.492\n",
      "st         31.115\n",
      "yes        31.001\n",
      "enemy      30.698\n",
      "lord       27.465\n",
      "horse      27.057\n",
      "young      26.839\n",
      "\n",
      "Top terms in corpus two\n",
      "mrs        -27.524\n",
      "mother     -26.640\n",
      "home       -19.883\n",
      "life       -19.837\n",
      "little     -19.442\n",
      "till       -18.657\n",
      "god        -18.374\n",
      "soul       -18.098\n",
      "wife       -17.237\n",
      "answered   -17.148\n"
     ]
    }
   ],
   "source": [
    "# Compare Law and Politics-Government-Activism occupations\n",
    "\n",
    "print(\"Compare Law and Politics-Government-Activism occupations\")\n",
    "print(\"Corpus one: Law\")\n",
    "print(\"Corpus two: Politics-Government-Activism\")\n",
    "print()\n",
    "informative = fw.bayes_compare_language(\n",
    "    l1=[0],\n",
    "    l2=[2],\n",
    "    features=vectorized_text,\n",
    "    cv=prefw_vectorizer,\n",
    ")\n",
    "display_fw(informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcVkQRfkuqDM",
    "outputId": "20aab867-90a8-4496-9e71-466a6c133f05",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Law and Church occupations\n",
      "Corpus one: Law\n",
      "Corpus two: Church\n",
      "\n",
      "Vocab size is 64188\n",
      "Comparing language...\n",
      "Top terms in corpus one\n",
      "sir        35.416\n",
      "eyes       29.607\n",
      "general    27.349\n",
      "lady       26.932\n",
      "captain    26.398\n",
      "enemy      24.140\n",
      "yes        23.082\n",
      "young      22.964\n",
      "head       21.790\n",
      "colonel    21.783\n",
      "\n",
      "Top terms in corpus two\n",
      "god        -36.160\n",
      "people     -27.380\n",
      "ye         -26.825\n",
      "church     -26.235\n",
      "mrs        -25.409\n",
      "mother     -25.098\n",
      "home       -24.761\n",
      "till       -23.857\n",
      "school     -21.529\n",
      "children   -21.026\n"
     ]
    }
   ],
   "source": [
    "# Compare Law and Church occupations\n",
    "\n",
    "print(\"Compare Law and Church occupations\")\n",
    "print(\"Corpus one: Law\")\n",
    "print(\"Corpus two: Church\")\n",
    "print()\n",
    "informative = fw.bayes_compare_language(\n",
    "    l1=[0],\n",
    "    l2=[1],\n",
    "    features=vectorized_text,\n",
    "    cv=prefw_vectorizer,\n",
    ")\n",
    "display_fw(informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p8caO9PuqDN",
    "outputId": "a355273f-5b78-4122-c9d9-27a17643ebe7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Church and Politics-Government-Activism occupations\n",
      "Corpus one: Church\n",
      "Corpus two: Politics-Government-Activism\n",
      "\n",
      "Vocab size is 64188\n",
      "Comparing language...\n",
      "Top terms in corpus one\n",
      "ye         32.085\n",
      "god        29.460\n",
      "church     25.853\n",
      "thee       24.521\n",
      "lord       24.277\n",
      "christ     21.931\n",
      "thy        21.407\n",
      "sir        21.089\n",
      "barton     20.887\n",
      "got        20.877\n",
      "\n",
      "Top terms in corpus two\n",
      "replied    -19.238\n",
      "gray       -18.762\n",
      "laura      -15.944\n",
      "eyes       -15.766\n",
      "dick       -15.068\n",
      "reuben     -12.761\n",
      "lady       -12.455\n",
      "sculptor   -12.121\n",
      "answered   -12.078\n",
      "paris      -11.877\n"
     ]
    }
   ],
   "source": [
    "# Compare Church and Politics-Government-Activism occupations\n",
    "\n",
    "print(\"Compare Church and Politics-Government-Activism occupations\")\n",
    "print(\"Corpus one: Church\")\n",
    "print(\"Corpus two: Politics-Government-Activism\")\n",
    "print()\n",
    "informative = fw.bayes_compare_language(\n",
    "    l1=[1],\n",
    "    l2=[2],\n",
    "    features=vectorized_text,\n",
    "    cv=prefw_vectorizer,\n",
    ")\n",
    "display_fw(informative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY3Hl-REuqDN"
   },
   "source": [
    "Based on the results from fightinwords, we see that the authors who work in Church indeed refer a lot to religious elements, such as god, lord, and church. For authors in Law, they write a lot about colonel, captain, etc. For authors in Politics-Government-Activism, they write a lot about people, as names such as laura and dick and nouns like mother and wife are frequent words, and places, such as paris. Religious words such as \"lord\" do appear in both Law and Politics-Government-Activism texts, but it is difficult to determine whether these words are used in a religious context, since they can mean different things under various contexts. Both authors from Law and Politics-Government-Activism have an inclination to write about \"eyes\", \"colonel\", etc. I am interested in whether authors from these two fields might have similar preferences in what they write. From this step forward, I will employ a BERT model to predict whether texts could be correctly predicted to be written by authors from Law or Politics-Government-Activism based on the words. I will process the occupation labels into binary values: 0 for Law and 1 for Politics-Government-Activism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vqkPKT48uqDN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BERT model\n",
    "# process labels: Law to 0 and Politics-Government-Activism to 1\n",
    "binary = []\n",
    "for item in labels:\n",
    "    if item == \"Law\":\n",
    "        binary.append(0)\n",
    "    elif item == \"Politics-Government-Activism\":\n",
    "        binary.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QvGMAsxhuqDN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.01,\n",
    "    max_df=0.9,\n",
    "    use_idf=True,\n",
    "    strip_accents='unicode',\n",
    "    input='content'\n",
    ")\n",
    "\n",
    "bert_df = {}\n",
    "for key, value in texts.items():\n",
    "    if key in politics_names:\n",
    "        bert_df[key] = value\n",
    "    elif key in law_names:\n",
    "        bert_df[key] = value\n",
    "\n",
    "vectorized_df = vectorizer.fit_transform(bert_df.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1erL079uqDO",
    "outputId": "00f21797-635c-4d30-e797-ed0a004723b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of resulting feature matrix: (80, 52859)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of resulting feature matrix:\", vectorized_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5_LrNRjXuqDO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split train and test datasets\n",
    "\n",
    "X_input = list(bert_df.values())\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    X_input, binary,\n",
    "    test_size=0.33, shuffle=True,\n",
    "    stratify=binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGJ_AyEKNBiV"
   },
   "source": [
    "When adjusting the parameters for train_test_split, I defined the stratify parameter to ensure that the splitting of the dataset maintains the proportion of classes between the train and test sets. This helps avoid potential biase in the model training. With inproportioned datasizes, the model might be trained to predict the class with greater size, thus producing inaccurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hay3thM8uqDO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mEvwcPGuqDP",
    "outputId": "052a4d4d-1541-4e75-abd8-212644c9de28",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.51336898 0.55322455 0.46363636 0.6        0.68      ]\n",
      "Mean F1 score: 0.5620459801636273\n",
      "\n",
      "Evaluation of the accuracy the gaussian naive bayes model's predictions match up with the labels for the poems: \n",
      "\n",
      "0.5990696578931873\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.38      0.35         8\n",
      "           1       0.72      0.68      0.70        19\n",
      "\n",
      "    accuracy                           0.59        27\n",
      "   macro avg       0.53      0.53      0.53        27\n",
      "weighted avg       0.61      0.59      0.60        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline model with GaussianNB\n",
    "\n",
    "gauss = GaussianNB().fit(X_train.toarray(), train_labels)\n",
    "\n",
    "gauss_scores = cross_val_score(gauss, X_train.toarray(), train_labels, cv=5, scoring='f1_weighted')\n",
    "\n",
    "print(\"Cross-validation scores:\", gauss_scores)\n",
    "print(\"Mean F1 score:\", gauss_scores.mean())\n",
    "\n",
    "y_pred = gauss.predict(X_test.toarray())\n",
    "\n",
    "print(\"\\nEvaluation of the accuracy the gaussian naive bayes model's predictions match up with the labels for the poems: \\n\")\n",
    "print(f1_score(test_labels, y_pred, average='weighted'))\n",
    "print(\"\\nClassification report: \\n\")\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBFXXrSPc1-R"
   },
   "source": [
    "For the baseline model, I experimented with three classifiers: GaussianNB, svm, and MLPClassifier (neural_network) to get the best baseline classifier. Since for me, the classes 0 and 1 are equally important, I do not want f1-score of 0 for either classes 0 or 1. Among the three classifiers, GaussianNB had cross validated f1-score 0.49, svm had cross validated f1-score 0.58, and MLPClassifier had cross validated f1-score 0.54. Although it seems that GaussianNB performed the worst, the fact that its f1-score for class 0 is not 0 is what I wanted. Also, GaussianNB works well with smaller datasets, so I decided to use GaussianNB as my baseline classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "AuQPmpEUuqDQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "device_name = 'cuda'\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "cached_model_directory_name = 'distilbert-reviews-genres'\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TPC9ODssuqDQ"
   },
   "outputs": [],
   "source": [
    "unique_labels = set(lab for lab in binary)\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ojO1-BOcuqDR"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings  = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "train_labels_encoded = [label2id[y] for y in train_labels]\n",
    "test_labels_encoded  = [label2id[y] for y in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "iCN_-wVWuqDR"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "G3eGKQFMuqDR"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
    "test_dataset = MyDataset(test_encodings, test_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjUHPwr9uqDR",
    "outputId": "e6929666-00f6-4fa1-ad4b-3828e546a80b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained BERT model\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(id2label)\n",
    ").to(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2ofeyePPuqDS"
   },
   "outputs": [],
   "source": [
    "# fine tune the BERT model\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1).flatten().tolist()\n",
    "  # score = accuracy_score(labels, preds)\n",
    "  score = f1_score(labels, preds, average='weighted')\n",
    "  return {\n",
    "      'f1': score,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujY5RFJKuqDX"
   },
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TKSaTZ8ChTB1",
    "outputId": "573ed65c-a108-43af-f9a7-8c31f17ffea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.608258</td>\n",
       "      <td>0.581320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.648996</td>\n",
       "      <td>0.562963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.795454</td>\n",
       "      <td>0.560489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>1.089133</td>\n",
       "      <td>0.560489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>1.349090</td>\n",
       "      <td>0.560489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.465461</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.544039</td>\n",
       "      <td>0.534921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.588499</td>\n",
       "      <td>0.534921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.607944</td>\n",
       "      <td>0.534921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.613111</td>\n",
       "      <td>0.534921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 1: 0.7\n",
      "Training Fold 2/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>2.731807</td>\n",
       "      <td>0.543771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.115401</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.106165</td>\n",
       "      <td>0.535895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.329698</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.571225</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.700755</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.768959</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.807746</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.828524</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.835279</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 2: 0.6842105263157895\n",
      "Training Fold 3/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.237554</td>\n",
       "      <td>0.534921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.616206</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.925040</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.095928</td>\n",
       "      <td>0.508642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.139961</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.182985</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.245332</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.271626</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.287488</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.293525</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 3: 0.7027027027027027\n",
      "Training Fold 4/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.709625</td>\n",
       "      <td>0.508642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.926248</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.123736</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.270529</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.337757</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.395114</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.442504</td>\n",
       "      <td>0.535895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.464344</td>\n",
       "      <td>0.535895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.472281</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.476185</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 4: 0.6666666666666667\n",
      "Training Fold 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.583357</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.695212</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.788825</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.860310</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.907539</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.938230</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.961478</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.974825</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.982724</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.985763</td>\n",
       "      <td>0.599070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 5: 0.7027027027027027\n",
      "\n",
      "Average F1-Accuracy: 0.6912565196775724\n"
     ]
    }
   ],
   "source": [
    "# baseline model with StratiedKFold\n",
    "\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(X_train, train_labels)):\n",
    "  print(f\"Training Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=56, shuffle=True)\n",
    "  val_loader = DataLoader(test_dataset, batch_size=56, shuffle=False)\n",
    "  device = torch.device('cuda')\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=11,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    learning_rate=5e-5,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=3,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
    "    compute_metrics=compute_metrics      # our custom evaluation function\n",
    ")\n",
    "\n",
    "  trainer.train()\n",
    "\n",
    "  predicts = trainer.predict(test_dataset)\n",
    "\n",
    "  # print(predicted_labels)\n",
    "  # print(f1_score(test_labels, predicted_labels))\n",
    "  preds = np.argmax(predicts.predictions, axis=-1)\n",
    "\n",
    "  # print(trainer.predict(test_texts).label_ids) # val_predictions\n",
    "\n",
    "  fold_accuracy = f1_score(test_labels, preds)\n",
    "  fold_accuracies.append(fold_accuracy)\n",
    "  print(f\"Accuracy for Fold {fold+1}: {fold_accuracy}\")\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "print()\n",
    "print(f\"Average F1-Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PguNl9nchWa2",
    "outputId": "33791e24-ab6a-4348-a8c1-08183a3284cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.38      0.35         8\n",
      "           1       0.72      0.68      0.70        19\n",
      "\n",
      "    accuracy                           0.59        27\n",
      "   macro avg       0.53      0.53      0.53        27\n",
      "weighted avg       0.61      0.59      0.60        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at weighted f1 and accuracy\n",
    "print(\"Classification report: \\n\")\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "c0pnY8-ThZXj"
   },
   "outputs": [],
   "source": [
    "  predicted_labels = predicts.predictions.argmax(-1) # Get the highest probability prediction\n",
    "  predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "  predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYBnnIWahbjI",
    "outputId": "85c59905-e73d-40b8-96a0-3eeb7ae2657a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct examination(s):\n",
      "\n",
      "LABEL: 1\n",
      "REVIEW TEXT: BERNARD LILE.\n",
      "CHAPTER I.\n",
      "\"I can bear\n",
      "However wretchedly, 'tis still to bear\n",
      "In life what others co ...\n",
      "\n",
      "LABEL: 1\n",
      "REVIEW TEXT: DOCTOR JOHNS.\n",
      "I.\n",
      "IN the summer of 1812, when the good people of Connecticut were feeling uncommonly  ...\n",
      "\n",
      "LABEL: 1\n",
      "REVIEW TEXT: BELL BRANDON.\n",
      "A TALE OF NEW YORK IN 1810.\n",
      "THREE HUNDRED DOLLAR PRIZE STORY!\n",
      "CHAPTER I.\n",
      "IT was the bo ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correct examination\n",
    "print(\"Correct examination(s):\\n\")\n",
    "for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 5):\n",
    "  if _true_label == _predicted_label:\n",
    "    print('LABEL:', _true_label)\n",
    "    print('REVIEW TEXT:', _text[:100], '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IjmbRedhdYG",
    "outputId": "974e0928-497d-4e93-e3cd-eb624bcc657e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect examination(s):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# incorrect examination\n",
    "print(\"Incorrect examination(s):\\n\")\n",
    "for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, predicted_labels, test_texts)), 5):\n",
    "  if _true_label != _predicted_label:\n",
    "    print('TRUE LABEL:', _true_label)\n",
    "    print('PREDICTED LABEL:', _predicted_label)\n",
    "    print('REVIEW TEXT:', _text[:100], '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "XU4A4DkVhffn"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "genre_classifications_dict = defaultdict(int)\n",
    "for _true_label, _predicted_label in zip(test_labels, predicted_labels):\n",
    "  genre_classifications_dict[(_true_label, _predicted_label)] += 1\n",
    "\n",
    "dicts_to_plot = []\n",
    "for (_true_genre, _predicted_genre), _count in genre_classifications_dict.items():\n",
    "  dicts_to_plot.append({'True Genre': _true_genre,\n",
    "                        'Predicted Genre': _predicted_genre,\n",
    "                        'Number of Classifications': _count})\n",
    "\n",
    "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "df_wide = df_to_plot.pivot_table(index='True Genre',\n",
    "                                 columns='Predicted Genre',\n",
    "                                 values='Number of Classifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "D8eapeLphhTx",
    "outputId": "2424388e-55fb-4d53-c9d8-a7c1318ca780"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create heatmap graph for the results\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "sns.heatmap(df_wide, linewidths=1, cmap='Greens')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05QUbgfkxnYc"
   },
   "source": [
    "Based on the heatmap, we see that the correct prediction of class 1 has the highest f1 accuracy score, while it's the lowest for correctly predicting class 0. It is also important to acknowledge that the predictions and scores also depend on the train_test_split that was performed: different train/test datasets could generate different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7MVVH51uqDX"
   },
   "source": [
    "## 4. Discussion and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yt8pRd9uqDY"
   },
   "source": [
    "The average f1 accuracy of the prediction after using the BERT model is 0.69, which is higher than 0.59, the baseline performance, but not significantly. For this problem, since I examine the difference between Law and Politics-Government-Activism authors' writing preferences, I consider both class 0 - Law - and class 1 - Politics-Government-Activism - the same and do not consider the score of one class more important than the other. However, the difference between data sizes might affect the output, as discussed earlier, the size for Politics-Government-Activism is almost 2 times bigger than Law, which could lead to biase when training the model as the model would be more inclined to predict Politics-Government-Activism, which is greater in number. The small data size also might be the cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfGBtn0Swjx7"
   },
   "source": [
    "\n",
    "According to the fightinwords output, there are indeed differences between occupations such as Church and Law, but since the two occupations Politics-Government-Activism and Law are so closely associated with each other, we can speculate reasonably that the low performance score by BERT could be due to the similarities between topics that authors from these two occupations like to write. However, it is also important to take into account that the process data entails all texts from authors with the same occupation, which means even if some authors with Church occupation don't write about Church at "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDoSbsK-vZ18"
   },
   "source": [
    "Moreover, what I aim to explore in the second half of this exam using BERT is to explore the differences in writing preferences between authors in Law and Politics-Government-Activism authors from between 1789 and 1875. It's first important to acknowledge that what authors like to write solely depend on their personal preferences and might not have any connections with their occupation. The context of these texts might also affect the performance of the BERT model, as texts from earlier periods might have different word choices and meanings that differ from those BERT is trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K10YMPFOwtJ4"
   },
   "source": [
    "Some interesting projects that could be explored in the future include a more detailed study of a larger corpus of texts written by authors from Politics-Government-Activism and Law. Some interesting writing differences between the two occupations could potentially be discovered. It would also be helpful to study the backgrounds of authors from these two occupations to gain a better understanding of their writing preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLWNo-sCx-Ic"
   },
   "source": [
    "In conclusion, for authors whose primary occupation as Church, Politics-Government-Activism and Law do reflect their occupations in their preferred writing topic, according to fightinwords results, but whether or not their writings relate to each other should be studied in detail by examining individual texts and authors' religious beliefs. The texts written by authors with occupations in Politics-Government-Activism and Law seem to connect for their similar word choices, but the religious influences are not extremely explicit.\n",
    "\n",
    "By utilizing LLM-based model BERT, I attempted to distinguish texts written by authors in Politics-Government-Activism and Law. The performance was fine but not ideal. I speculate the primary reason for that could be the small corpus, imbalanced data, and similarities between the texts by the two occupations.\n",
    "\n",
    "Some interesting future projects relevant to this study include: writing preference similarities between authors who have occupations in similar areas and different occupations, difference between authors from Politics-Government-Activism and Law with larger corpus, and some specific context study of the United States between 1789 and 1875."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZA2Nytg7uqDF"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
